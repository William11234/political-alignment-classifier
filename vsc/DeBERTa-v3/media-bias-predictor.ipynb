{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🧠 Media Bias Classifier\n",
    "\n",
    "This project proposes a machine learning-based News Bias Detection System that analyses news articles to identify potential biases in reporting. By leveraging natural language processing (NLP) and explainable AI techniques, the system will assess textual content and detect linguistic patterns that indicate bias.\n",
    "\n",
    "Bias in news media can shape public opinion and influence decision-making, making it essential to recognize and mitigate biased reporting. Traditional methods of bias detection rely on human judgment, which is subjective, time-consuming, and inconsistent. This project aims to build an automated and transparent system for detecting bias, promoting more critical and informed media consumption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain Understanding for Detecting Media Bias\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "The ability to detect media bias is crucial in today's information-heavy world. The public relies on the media to make informed decisions about political, social, and economic matters. However, the way news is presented can greatly influence public perception. Bias in the media, whether political, ideological, or cultural, can distort the truth, influencing opinions and contributing to misinformation.\n",
    "\n",
    "In this section, we explore the domain of media bias detection and how various linguistic patterns and machine learning models can be used to detect bias in text. Understanding the context and the factors influencing media bias is essential for building an effective model.\n",
    "\n",
    "## 2. Societal Importance of Detecting Media Bias\n",
    "\n",
    "Media bias exists in every form of media — from news articles to social media posts. In the current era, misinformation and disinformation spread rapidly, and understanding bias in the media can help individuals make informed decisions. By detecting and analyzing bias, we allow readers to critically evaluate the sources of their information.\n",
    "\n",
    "### Key Reasons for Media Bias Detection:\n",
    "- **Informed Public**: People can make more informed decisions when they can identify bias in news articles.\n",
    "- **Transparency in Media**: Promotes trust in journalism by exposing unfair, biased reporting.\n",
    "- **Combating Misinformation**: Helps reduce the spread of misinformation by recognizing and challenging biased narratives.\n",
    "\n",
    "## 3. Bias in Media\n",
    "\n",
    "Bias is defined as the inclination or prejudice for or against one person, group, or idea, often in a way considered unfair. In the context of media, bias can manifest through wording, framing, and the selective inclusion or omission of information.\n",
    "\n",
    "### Types of Media Bias:\n",
    "- **Political Bias**: Favoring one political party or ideology over others.\n",
    "- **Ideological Bias**: Expressing prejudice based on certain ideologies, such as economic or cultural views.\n",
    "- **Sensationalism**: Using exaggerated language to attract attention, which can distort the true nature of events.\n",
    "\n",
    "## 4. Linguistic Analysis of Bias\n",
    "\n",
    "Linguistic patterns in news articles are key to understanding bias. The way in which a story is framed, the choice of words, and the structure of the sentences can all indicate bias. Through the use of NLP (Natural Language Processing), it's possible to uncover these patterns and detect subtle biases that may not be immediately obvious to a reader.\n",
    "\n",
    "### Key Linguistic Indicators of Bias:\n",
    "- **Word Choice**: Words like \"accused\" vs. \"guilty\" or \"terrorist\" vs. \"freedom fighter\" can reveal a source’s stance.\n",
    "- **Framing**: The angle from which the story is told. For example, focusing on the victim vs. focusing on the perpetrator.\n",
    "- **Omission**: Leaving out important context or facts that might change the interpretation of a story.\n",
    "\n",
    "## 5. Example of Linguistic Bias in Headlines\n",
    "\n",
    "Consider the following example comparing two headlines from right-leaning and left-leaning news outlets about a Tesla firebomber:\n",
    "\n",
    "**Right-leaning headline**: \"Attorney General Pam Bondi Announces Federal Charges Against Colorado Tesla Firebomber (VIDEO)\"  \n",
    "**Left-leaning headline**: \"Pam Bondi says she wants 20 years in prison for Colorado man accused of firebombing Tesla dealership\"\n",
    "\n",
    "### Analysis of Bias:\n",
    "- **Right-leaning**: The headline emphasizes Pam Bondi's official title (Attorney General), adding weight to the authority of the announcement. The use of the word \"firebomber\" presents the accused in a negative light, framing them as guilty.\n",
    "- **Left-leaning**: The headline omits Pam Bondi's title, reducing her perceived authority. The use of \"accused\" leaves room for doubt and emphasizes the need for a legal process. The focus is more on the alleged punishment than the severity of the crime.\n",
    "\n",
    "The difference in the choice of words — such as \"firebomber\" vs. \"accused\" — showcases how the same event can be framed in very different ways depending on political alignment.\n",
    "\n",
    "## 6. NLP Techniques for Media Bias Detection\n",
    "\n",
    "To detect bias, machine learning models, especially those in Natural Language Processing (NLP), can be employed. NLP helps computers understand the structure and meaning of human language, making it possible to detect patterns that may indicate bias.\n",
    "\n",
    "### Key NLP Approaches:\n",
    "- **TF-IDF (Term Frequency-Inverse Document Frequency)**: A traditional method for text representation, TF-IDF helps understand word importance in a given context.\n",
    "- **Transformers (e.g., BERT, DeBERTa)**: These models use contextual information from surrounding words, making them ideal for tasks like bias detection where context plays a crucial role.\n",
    "\n",
    "## 7. Models and Techniques Used\n",
    "\n",
    "### 7.1. **BERT (Bidirectional Encoder Representations from Transformers)**\n",
    "\n",
    "BERT is a transformer-based model that understands text by considering the context of all words in a sentence, rather than just a single word at a time. This bidirectional view enables BERT to capture deeper meaning and detect bias more accurately than earlier models.\n",
    "\n",
    "- **Masked Language Modeling**: BERT learns by predicting missing words in a sentence, which helps it understand how words relate to each other in context.\n",
    "- **Self-Attention Mechanism**: This allows BERT to focus on relevant parts of a sentence and understand how words influence each other.\n",
    "\n",
    "### 7.2. **DeBERTa (Decoding-enhanced BERT with disentangled attention)**\n",
    "\n",
    "DeBERTa is an advanced version of BERT that uses improved techniques for attention and decoding. It improves upon BERT by disentangling the attention mechanism, allowing it to better capture the relationships between words.\n",
    "\n",
    "- **Disentangled Attention**: Improves how the model relates words and their positions in a sentence.\n",
    "- **Enhanced Masking**: Makes the model more robust to ambiguity in language.\n",
    "\n",
    "## 8. **Ethical Implications**\n",
    "\n",
    "Detecting media bias with AI presents both opportunities and ethical concerns. While these systems can promote transparency and help users identify biased reporting, they also risk introducing or reinforcing biases themselves if not carefully designed and evaluated.\n",
    "\n",
    "Probably the three most important ways of causing societal harm using such a technology would be the lack of transparancy and accountability, misuse of the model, and aggresive use of bias detection.\n",
    "\n",
    "- Users must understand how the system works and why it labels content a certain way. A lack of explainability may reduce trust or mislead users.\n",
    "- Bias detection tools could be exploited to discredit valid journalism by labelling it as having some form of heavy political bias, depending on who controls the tool or how results are framed. The model is not 100% correct, it serves as a tool to potentially detect forms of bias for the user to remove it. However, as of this version of the project, there is no feedback given about what specifically is considered biased in a particular political direction in an article.\n",
    "- These tools are meant to inform people, but if used too strictly, they could silence different opinions and limit diversity in the media.\n",
    "\n",
    "## 9. Conclusion\n",
    "\n",
    "Detecting media bias is not just an important challenge in computational linguistics, but it also has real-world consequences for society. By using advanced NLP methods like BERT and DeBERTa, we can help people become more aware of the biases in the news they read and watch. In an era where information spreads rapidly and we’re more connected than ever, creating tools to identify and reduce bias is essential to ensuring we all have access to more balanced, \"objective\" media.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import json\n",
    "import unicodedata\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import Trainer, TrainingArguments, BertForSequenceClassification, BertTokenizer, TrainerCallback\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import torch\n",
    "from transformers import EarlyStoppingCallback\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📊 Preprocessing Functions\n",
    "\n",
    "These functions clean and prepare the input for **DeBERTa**. It ensures the model focusses on meaningful, consistent, and token-friendly content.\n",
    "\n",
    "- **🧱 Boilerplate Removal:** Removes generic news agency references (like \"According to CNN\" or \"As reported by Reuters\") to reduce noise in the input text.\n",
    "- - ✅ *Why it matters:* These phrases don’t help the model learn bias—they're common across sources and can act as noise.\n",
    "\n",
    "- **🔤 Accent Removal:** Strips accents from letters (e.g., “é” becomes “e”), improving consistency across languages. \n",
    "- - ✅ *Why it matters:* Helps reduce unnecessary vocabulary complexity for the tokenizer.\n",
    "\n",
    "- **🧼 Preprocess_for_bert:** Applies all the text cleaning: removes boilerplate, strips accents, removes non-ASCII chars, and lowercases it.\n",
    "- - ✅ *Why it matters:* Makes the text clean and simple for the tokenizer. This should be consistent throughout every text to avoid noise and bias.\n",
    "\n",
    "- **🧼 Preprocess_for_bert_title:** Same as the function above, but without lowercasing.\n",
    "- - ✅ *Why without lowercasing?* The title is shorter and often includes certain context through capitalization. For example: \n",
    "\n",
    "*May to lead Brexit talks*\n",
    "\n",
    "**Original:**\n",
    "Refers to Theresa May, the former UK prime minister.\n",
    "\n",
    "**Lowercased:**\n",
    "*may to lead brexit talks*\n",
    "Now it could look like it's using the month of May or the modal verb “may” — completely changes meaning or adds ambiguity.\n",
    "\n",
    "It might not look like a big deal to the human brain, but we have to be specific when training such a model.\n",
    "\n",
    "- **✂️ truncate_for_model:** Fits both title and content into a maximum token limit, prioritizing the title. The line *remaining = max_total_tokens - len(title_tokens) - 3* accounts for 1 [CLS] token (added automatically by tokenizer) 2 [SEP] tokens (between and after title/content). Then *content_tokens = content_tokens[:max(0, remaining)]* truncates the content if it doesn't fit.\n",
    "- - ✅ *Why it matters:* Transformers have a max token limit. This is making sure it never goes over that, but still prioritizes the title.\n",
    "\n",
    "- **🎯 compute_tfidf_scores:** Simple scoring function based on the average TF-IDF score of the article.\n",
    "- - ✅ *Why it matters:* Used for curriculum sampling (easy → hard progression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_boilerplate(text):\n",
    "    boilerplate_patterns = [\n",
    "        r\"(?:According to|As reported by|As per)?\\s?(CNN|Fox News|Reuters|BBC|The New York Times|AP|Washington Post)[,:\\s]*(reports|says|states)?\",\n",
    "        r\"(?:Reported by|From the article in)?\\s?(CNN|Fox News|Reuters|BBC|The New York Times|AP|Washington Post)\"\n",
    "    ]\n",
    "    for pattern in boilerplate_patterns:\n",
    "        text = re.sub(pattern, \"\", text, flags=re.IGNORECASE)\n",
    "    return text.strip()\n",
    "\n",
    "def remove_accents(text):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', text)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "def preprocess_for_bert(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = remove_boilerplate(text)\n",
    "    text = remove_accents(text)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "    return text.lower().strip()\n",
    "\n",
    "def preprocess_for_bert_title(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = remove_boilerplate(text)\n",
    "    text = remove_accents(text)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "def truncate_for_model(title, content, max_total_tokens=384):\n",
    "    title_tokens = tokenizer.tokenize(title)\n",
    "    content_tokens = tokenizer.tokenize(content)\n",
    "    \n",
    "    remaining = max_total_tokens - len(title_tokens) - 3  # [CLS], [SEP], [SEP]\n",
    "    content_tokens = content_tokens[:max(0, remaining)]\n",
    "    \n",
    "    return f\"Title: {tokenizer.convert_tokens_to_string(title_tokens)} Content: {tokenizer.convert_tokens_to_string(content_tokens)}\"\n",
    "\n",
    "def compute_tfidf_scores(texts):\n",
    "    \"\"\"\n",
    "    Returns average TF-IDF score for each text.\n",
    "    Lower score = more ambiguous, so harder to classify.\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer(max_features=10000)\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "    mean_scores = tfidf_matrix.mean(axis=1)\n",
    "    return np.array(mean_scores).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeBERTa-v3\n",
    "\n",
    "**DeBERTa-v3 (Decoding-enhanced BERT with disentangled attention)** is an advanced NLP model developed by Microsoft. It improves upon the original BERT by incorporating two key innovations: disentangled attention and enhanced decoding. Disentangled attention allows the model to better capture the relationship between words and their positions, while enhanced decoding improves the model's understanding of context. These advancements enable DeBERTa-v3 to outperform BERT and other models in many NLP tasks, providing more accurate results for complex language understanding.\n",
    "\n",
    "### 🧠 1. Disentangled Attention: Better Word + Position Understanding\n",
    "In BERT, word content and position are combined into a single embedding before being passed to the self-attention layers. This means the model might not always clearly separate what a word means from where it appears in the sentence.\n",
    "\n",
    "DeBERTa splits the meaning of words from their positions, allowing it to analyze how word placement affects meaning more precisely.\n",
    "\n",
    "Example:\n",
    "\n",
    "\"Only Alice punched Bob.\"\n",
    "\n",
    "\"Bob punched only Alice.\"\n",
    "\n",
    "These sentences use the same words, but mean very different things.\n",
    "DeBERTa's disentangled attention understands the difference better than BERT, which mixes meaning and position too early.\n",
    "\n",
    "🧩 This helps the model recognize who did what, and how words like \"only\" change that meaning depending on position.\n",
    "\n",
    "### 🔁 2. Enhanced Mask Decoder: Sharper Context Grasp\n",
    "In BERT, the model predicts masked tokens (like [MASK]) using a simple linear layer on top of the encoder output. A simple linear layer just takes the number-y output for a word from BERT and runs it through a math formula (matrix multiplication + bias) to guess what the masked word should be. It’s like plugging the word's info into a calculator that spits out a score for every word in the dictionary, then picking the one with the highest score.\n",
    "\n",
    "DeBERTa's decoder uses more contextual information than BERT to fill in missing or masked words during training.\n",
    "\n",
    "Example:\n",
    "\n",
    "\"The cat sat on the [MASK].\"\n",
    "\n",
    "🔍 DeBERTa looks deeper into:\n",
    "\n",
    "- Grammatical structure\n",
    "- Likely surfaces a cat sits on\n",
    "- Sentence semantics\n",
    "\n",
    "It might confidently predict \"mat\", while BERT might guess less precisely (e.g., “sofa”, “floor”).\n",
    "\n",
    "📚 This makes DeBERTa more accurate for tasks that rely on subtle language cues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔎 Analytic Approach\n",
    "\n",
    "Detecting bias in articles might be an unclear objective, due to its abstraction. The first question to give attention to could be how exactly we detect bias, but there might be a better way to approach this. Instead, asking what bias is and how we should define it could provide more clarity.\n",
    "\n",
    "**Multi-Class Classification** is presumably the better option. It eliminates the need for something to be classified as unbiased, as the left, center, and right all contain some form of bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json_folder = r\"C:\\Fontys\\Semester4\\MediaBias_Predictor\\Article-Bias-Prediction-main\\data\\jsons\"\n",
    "\n",
    "# data = []\n",
    "\n",
    "# # Loop through JSON files\n",
    "# for filename in os.listdir(json_folder):\n",
    "#     if filename.endswith(\".json\"):\n",
    "#         file_path = os.path.join(json_folder, filename)\n",
    "#         with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#             article = json.load(f)\n",
    "            \n",
    "#             # Extract only the fields present in the JSON\n",
    "#             data.append({\n",
    "#                 \"ID\": article.get(\"ID\", \"\"),  \n",
    "#                 \"topic\": article.get(\"topic\", \"\"),  \n",
    "#                 \"source\": article.get(\"source\", \"\"),  \n",
    "#                 \"title\": article.get(\"title\", \"\"),  \n",
    "#                 \"date\": article.get(\"date\", \"\"),  \n",
    "#                 \"authors\": article.get(\"authors\", \"\"),  \n",
    "#                 \"content\": article.get(\"content\", \"\"),  \n",
    "#                 \"bias_text\": article.get(\"bias_text\", \"\"),  # Bias category (e.g., left, center, right)\n",
    "#                 \"url\": article.get(\"url\", \"\"),  \n",
    "#                 \"source_url\": article.get(\"source_url\", \"\")  \n",
    "#             })\n",
    "\n",
    "df = pd.read_csv('../news_bias_data.csv')\n",
    "\n",
    "\n",
    "# df.to_csv(\"news_bias_data.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>topic</th>\n",
       "      <th>source</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>authors</th>\n",
       "      <th>content</th>\n",
       "      <th>bias_text</th>\n",
       "      <th>url</th>\n",
       "      <th>source_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>004Gt3gcsotuiYmz</td>\n",
       "      <td>terrorism</td>\n",
       "      <td>New York Times - News</td>\n",
       "      <td>Bomb Suspect Changed After Trip Abroad, Friend...</td>\n",
       "      <td>2016-09-20</td>\n",
       "      <td>N. R. Kleinfield</td>\n",
       "      <td>Besides his most recent trip to Quetta , Mr. R...</td>\n",
       "      <td>left</td>\n",
       "      <td>http://www.nytimes.com/2016/09/20/nyregion/ahm...</td>\n",
       "      <td>www.nytimes.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00eP4XD3VdMmHITE</td>\n",
       "      <td>supreme_court</td>\n",
       "      <td>Vox</td>\n",
       "      <td>Why Susan Collins claims she’s being bribed ov...</td>\n",
       "      <td>2018-09-12</td>\n",
       "      <td>Emily Stewart, Terry Nguyen, Rebecca Jennings,...</td>\n",
       "      <td>Is Maine Republican Sen. Susan Collins being b...</td>\n",
       "      <td>left</td>\n",
       "      <td>https://www.vox.com/policy-and-politics/2018/9...</td>\n",
       "      <td>www.vox.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00FTGIZEd6B8zQ4U</td>\n",
       "      <td>education</td>\n",
       "      <td>Ezra Klein</td>\n",
       "      <td>Poll: Prestigious Colleges Won't Make You Happ...</td>\n",
       "      <td>2014-05-06</td>\n",
       "      <td>Anya Kamenetz</td>\n",
       "      <td>Poll : Prestigious Colleges Wo n't Make You Ha...</td>\n",
       "      <td>left</td>\n",
       "      <td>http://www.npr.org/blogs/thetwo-way/2014/05/06...</td>\n",
       "      <td>www.npr.org</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00HGGqBRf1kzPRlg</td>\n",
       "      <td>us_house</td>\n",
       "      <td>Breitbart News</td>\n",
       "      <td>Paul Ryan Reportedly Says No Chance for Border...</td>\n",
       "      <td>2017-09-12</td>\n",
       "      <td>Ian Mason</td>\n",
       "      <td>House Speaker Paul Ryan , at a private dinner ...</td>\n",
       "      <td>right</td>\n",
       "      <td>http://www.breitbart.com/big-government/2017/0...</td>\n",
       "      <td>www.breitbart.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00IzI5ynahBVtC9l</td>\n",
       "      <td>white_house</td>\n",
       "      <td>Guest Writer - Left</td>\n",
       "      <td>OPINION: Trump seeking change of legal fortune...</td>\n",
       "      <td>2019-07-11</td>\n",
       "      <td>Analysis Stephen Collinson</td>\n",
       "      <td>( CNN ) President Donald Trump has reason to h...</td>\n",
       "      <td>left</td>\n",
       "      <td>https://www.cnn.com/2019/07/11/politics/donald...</td>\n",
       "      <td>www.cnn.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ID          topic                 source  \\\n",
       "0  004Gt3gcsotuiYmz      terrorism  New York Times - News   \n",
       "1  00eP4XD3VdMmHITE  supreme_court                    Vox   \n",
       "2  00FTGIZEd6B8zQ4U      education             Ezra Klein   \n",
       "3  00HGGqBRf1kzPRlg       us_house         Breitbart News   \n",
       "4  00IzI5ynahBVtC9l    white_house    Guest Writer - Left   \n",
       "\n",
       "                                               title        date  \\\n",
       "0  Bomb Suspect Changed After Trip Abroad, Friend...  2016-09-20   \n",
       "1  Why Susan Collins claims she’s being bribed ov...  2018-09-12   \n",
       "2  Poll: Prestigious Colleges Won't Make You Happ...  2014-05-06   \n",
       "3  Paul Ryan Reportedly Says No Chance for Border...  2017-09-12   \n",
       "4  OPINION: Trump seeking change of legal fortune...  2019-07-11   \n",
       "\n",
       "                                             authors  \\\n",
       "0                                   N. R. Kleinfield   \n",
       "1  Emily Stewart, Terry Nguyen, Rebecca Jennings,...   \n",
       "2                                      Anya Kamenetz   \n",
       "3                                          Ian Mason   \n",
       "4                         Analysis Stephen Collinson   \n",
       "\n",
       "                                             content bias_text  \\\n",
       "0  Besides his most recent trip to Quetta , Mr. R...      left   \n",
       "1  Is Maine Republican Sen. Susan Collins being b...      left   \n",
       "2  Poll : Prestigious Colleges Wo n't Make You Ha...      left   \n",
       "3  House Speaker Paul Ryan , at a private dinner ...     right   \n",
       "4  ( CNN ) President Donald Trump has reason to h...      left   \n",
       "\n",
       "                                                 url         source_url  \n",
       "0  http://www.nytimes.com/2016/09/20/nyregion/ahm...    www.nytimes.com  \n",
       "1  https://www.vox.com/policy-and-politics/2018/9...        www.vox.com  \n",
       "2  http://www.npr.org/blogs/thetwo-way/2014/05/06...        www.npr.org  \n",
       "3  http://www.breitbart.com/big-government/2017/0...  www.breitbart.com  \n",
       "4  https://www.cnn.com/2019/07/11/politics/donald...        www.cnn.com  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧹 Cleaning contents and titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>cleaned_title</th>\n",
       "      <th>cleaned_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bomb Suspect Changed After Trip Abroad, Friend...</td>\n",
       "      <td>Besides his most recent trip to Quetta , Mr. R...</td>\n",
       "      <td>Bomb Suspect Changed After Trip Abroad, Friend...</td>\n",
       "      <td>besides his most recent trip to quetta , mr. r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why Susan Collins claims she’s being bribed ov...</td>\n",
       "      <td>Is Maine Republican Sen. Susan Collins being b...</td>\n",
       "      <td>Why Susan Collins claims shes being bribed ove...</td>\n",
       "      <td>is maine republican sen. susan collins being b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Poll: Prestigious Colleges Won't Make You Happ...</td>\n",
       "      <td>Poll : Prestigious Colleges Wo n't Make You Ha...</td>\n",
       "      <td>Poll: Prestigious Colleges Won't Make You Hpie...</td>\n",
       "      <td>poll : prestigious colleges wo n't make you hp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Paul Ryan Reportedly Says No Chance for Border...</td>\n",
       "      <td>House Speaker Paul Ryan , at a private dinner ...</td>\n",
       "      <td>Paul Ryan Reportedly Says No Chance for Border...</td>\n",
       "      <td>house speaker paul ryan , at a private dinner ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OPINION: Trump seeking change of legal fortune...</td>\n",
       "      <td>( CNN ) President Donald Trump has reason to h...</td>\n",
       "      <td>OPINION: Trump seeking change of legal fortune...</td>\n",
       "      <td>() president donald trump has reason to hope h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Bomb Suspect Changed After Trip Abroad, Friend...   \n",
       "1  Why Susan Collins claims she’s being bribed ov...   \n",
       "2  Poll: Prestigious Colleges Won't Make You Happ...   \n",
       "3  Paul Ryan Reportedly Says No Chance for Border...   \n",
       "4  OPINION: Trump seeking change of legal fortune...   \n",
       "\n",
       "                                             content  \\\n",
       "0  Besides his most recent trip to Quetta , Mr. R...   \n",
       "1  Is Maine Republican Sen. Susan Collins being b...   \n",
       "2  Poll : Prestigious Colleges Wo n't Make You Ha...   \n",
       "3  House Speaker Paul Ryan , at a private dinner ...   \n",
       "4  ( CNN ) President Donald Trump has reason to h...   \n",
       "\n",
       "                                       cleaned_title  \\\n",
       "0  Bomb Suspect Changed After Trip Abroad, Friend...   \n",
       "1  Why Susan Collins claims shes being bribed ove...   \n",
       "2  Poll: Prestigious Colleges Won't Make You Hpie...   \n",
       "3  Paul Ryan Reportedly Says No Chance for Border...   \n",
       "4  OPINION: Trump seeking change of legal fortune...   \n",
       "\n",
       "                                     cleaned_content  \n",
       "0  besides his most recent trip to quetta , mr. r...  \n",
       "1  is maine republican sen. susan collins being b...  \n",
       "2  poll : prestigious colleges wo n't make you hp...  \n",
       "3  house speaker paul ryan , at a private dinner ...  \n",
       "4  () president donald trump has reason to hope h...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"cleaned_title\"] = df[\"title\"].apply(preprocess_for_bert_title)\n",
    "df[\"cleaned_content\"] = df[\"content\"].apply(preprocess_for_bert)\n",
    "\n",
    "df[[\"title\", \"content\", \"cleaned_title\", \"cleaned_content\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ℹ️ Splitting the data\n",
    "\n",
    "This cell below handles the data splitting to evenly distribute the left, center, and right-winged data. This is used if the model needs to be trained on less data, so that even if you take a portion of the dataset, it is still evenly distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37554, 12)\n"
     ]
    }
   ],
   "source": [
    "features = [\"title\", \"content\", \"bias_text\"]\n",
    "\n",
    "\"\"\"left_df = df[df[\"bias_text\"] == \"left\"]\n",
    "center_df = df[df[\"bias_text\"] == \"center\"]\n",
    "right_df = df[df[\"bias_text\"] == \"right\"]\n",
    "\n",
    "total_samples = 5\n",
    "\n",
    "# Randomly sample from each class\n",
    "left_sampled = left_df.sample(n=total_samples, random_state=42)\n",
    "center_sampled = center_df.sample(n=total_samples, random_state=42)\n",
    "right_sampled = right_df.sample(n=total_samples, random_state=42)\n",
    "\n",
    "df = pd.concat([left_sampled, center_sampled, right_sampled])\n",
    "\n",
    "# Shuffle the final dataset\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\"\"\"\n",
    "\n",
    "df[features].head()\n",
    "# print(left_df.shape, center_df.shape, right_df.shape)\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **df.sample(frac=1, random_state=42)** means:\n",
    "\n",
    "- - **frac=1** → take 100% of the DataFrame, but in random order.\n",
    "\n",
    "- - **random_state=42** → makes the randomness repeatable, so every time you run it, it shuffles in the same way (helpful for debugging or reproducibility).\n",
    "\n",
    "- **.reset_index(drop=True)**:\n",
    "\n",
    "- - After shuffling, the original row indices are scrambled too.\n",
    "\n",
    "- - **reset_index(drop=True)** gives the DataFrame new clean row numbers (0, 1, 2, ...) and drops the old indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID                    0\n",
      "topic                 0\n",
      "source                0\n",
      "title                 0\n",
      "date               4407\n",
      "authors            9668\n",
      "content               0\n",
      "bias_text             0\n",
      "url                   0\n",
      "source_url            0\n",
      "cleaned_title         0\n",
      "cleaned_content       0\n",
      "dtype: int64\n",
      "bias_text\n",
      "right     13734\n",
      "left      13005\n",
      "center    10815\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Dataset after removing missing values:\n",
      "(37554, 12)\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Check the distribution of bias_text labels\n",
    "print(df['bias_text'].value_counts())\n",
    "\n",
    "print(\"\\nDataset after removing missing values:\")\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **BERT** model requires much less data preprocessing, since it is pre-trained on **raw text**. It is in fact helpful to include the things that we otherwise exclude from other models, because it provides a certain context that BERT recognizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias_text</th>\n",
       "      <th>label_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>left</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>left</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>left</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>right</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>left</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  bias_text  label_id\n",
       "0      left         1\n",
       "1      left         1\n",
       "2      left         1\n",
       "3     right         2\n",
       "4      left         1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "df[\"label_id\"] = label_encoder.fit_transform(df[\"bias_text\"]) # Representing left, center, right as 0, 1, 2\n",
    "\n",
    "df[[\"bias_text\", \"label_id\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ↗️ Tokenization\n",
    "\n",
    "Before text can go into DeBERTa, it has to be converted into numbers (tokens).\n",
    "\n",
    "- It Splits text into subwords (e.g., \"unbelievable\" → \"un\", \"##believable\").\n",
    "\n",
    "- Maps words/subwords to token IDs that DeBERTa understands.\n",
    "\n",
    "- Pads or truncates text to a fixed length (so batches are consistent).\n",
    "\n",
    "- Returns attention masks (to tell the model which tokens are real vs. padding).\n",
    "\n",
    "Two example sentences:\n",
    "\n",
    "- *L, did you know* (5 tokens)\n",
    "- *Gods of death love apples!* (6 tokens)\n",
    "\n",
    "If we set the max_length to be 8, it will allow a maximum of 8 tokens. If a piece of text has more than that, it truncates it down to 8, and if it has less, it adds padding to get it to 8. In the example sentences, it would be represented as follows:\n",
    "\n",
    "- *L, did you know [PAD] [PAD] [PAD]* (8 tokens)\n",
    "- *Gods of death love apples! [PAD] [PAD]* (8 tokens)\n",
    "\n",
    "The **attention mask** is like a signal for the model to know which tokens are real words and which ones are padding (which should be ignored). Padding is displayed as 0 and real words as 1.\n",
    "\n",
    "The attention mask for *L, did you know [PAD] [PAD] [PAD]* would look like *[1, 1, 1, 1, 1, 0, 0, 0]*.\n",
    "\n",
    "#### Why is this important?\n",
    "Deep learning models like DeBERTa process data in batches. Having all sentences the same length allows the model to process them simultaneously. If sentences have different lengths, the model would need to deal with each sentence one by one. Computers use matrices to process data efficiently. If sentences have different lengths, you can't create a matrix because the rows (sentences) won't have the same number of columns (tokens)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📚 Training Pipeline\n",
    "\n",
    "### 📊 NewsDataset class:\n",
    "\n",
    "- __init__: Stores the tokenized input (encodings) and the labels.\n",
    "\n",
    "- __len__: Returns the total number of samples in the dataset. Required so the model knows how many items to expect.\n",
    "\n",
    "- __getitem__: When the model asks for a specific item (e.g., the 5th article), this method: Converts the data into PyTorch tensors and returns a dictionary with the correct format: input_ids, attention_mask, and labels\n",
    "\n",
    "`item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}`\n",
    "This line is creating a new dictionary called item. It goes through each key-value pair in self.encodings, which is a dictionary containing the tokenized input data.\n",
    "`clone().detach()` is used to create a new tensor that's independent of the original one. It is like a copy to not mess with the original one.\n",
    "\n",
    "#### Tensors\n",
    "\n",
    "Tensors can be seen as multi-dimensional arrays:\n",
    "\n",
    "- Scalar (0D) -> 3,14 -> torch.tensor(3.14)\n",
    "- Vector (1D) -> [1, 2, 3] -> torch.tensor([1, 2, 3])\n",
    "- Matrix (2D) -> [[1, 2], [3, 4]] -> torch.tensor([[1, 2], [3, 4]])\n",
    "- Tensor (3D+) -> [[[...], [...]], [[...], [...]]] -> Used in images, NLP, etc.\n",
    "\n",
    "In NLP with BERT:\n",
    "\n",
    "A sentence becomes a 1D tensor (list of token IDs)\n",
    "\n",
    "A batch of sentences becomes a 2D tensor (each row is a sentence)\n",
    "\n",
    "### 📋 CurriculumTrainer class\n",
    "\n",
    "- **get_train_dataloader**: This method is responsible for returning the dataloader used during training. Normally, the dataloader would shuffle training data, but here we turn that off to preserve the \"easy-to-hard\" order required for curriculum learning.\n",
    "\n",
    "- **collate_fn**: The collate_fn is a function used to combine or “batch” data into a single batch. It is needed when you have different types of data, or the data isn't structured in a way that can be directly fed into the model. It makes sure that each input in the batch is the same size. The tokenizer does this too by adding padding to the text. The collate_fn function does the same, but provides more control.\n",
    "\n",
    "- **drop_last=False**: When dividing data into batches, sometimes you end up with a smaller batch at the end because the total data isn't a perfect multiple of the batch size. This setting says makes sure to keep that batch. Even though it's smaller, it still gets fed into the model.\n",
    "\n",
    "### 🤔 StratifiedKFold\n",
    "**K-Fold Cross-Validation** is a technique used in machine learning to evaluate the performance of a model. The dataset is divided into **k** equal-sized subsets or \"folds\". The model is trained on **k-1** folds and tested on the remaining fold. This is repeated **k** times, with each fold used exactly once for testing and the remaining ones for training. **Stratified K-Fold** ensures each fold has a similar distribution of the target labels, making it more reliable for evaluating models on imbalanced datasets. The dataset used already has evenly distributed labels, but it can still act as a safety net.\n",
    "\n",
    "### 👨‍🎓 Curriculum Learning\n",
    "\n",
    "Instead of feeding the model data randomly, curriculum learning starts with easier examples first, and moves on to harder ones later, because the model starts off being \"stupid\".\n",
    "\n",
    "##### 🔎 Line by line breakdown\n",
    "\n",
    "`train_data = [(titles[i], df[\"content\"].iloc[i], labels[i]) for i in train_index]` takes the training indices from the split. For each index it grabs the:\n",
    "- title\n",
    "- content\n",
    "- label\n",
    "\n",
    "It creates a list of tuples like:\n",
    "`[(\"Why something is great\", \"Something is amazing, because...\", 0), (...), ...]`\n",
    "\n",
    "`train_titles, train_contents, train_labels = zip(*train_data)` creates separate lists for all the titles, contents, and labels.\n",
    "\n",
    "`train_texts_full = [f\"{title} {content}\" for title, content in zip(train_titles, train_contents)]` combines the title and content into one text.\n",
    "\n",
    "`tfidf_scores = compute_tfidf_scores(train_texts_full)` High TF-IDF score → article is unique, has rare words. Low score → article is generic, uses common words.\n",
    "\n",
    "`train_data_with_scores = list(zip(train_titles, train_contents, train_labels, tfidf_scores))` now each sample has a score.\n",
    "\n",
    "`train_data_with_scores.sort(key=lambda x: x[3], reverse=True)` sorts the data using the fourth element of the tuple, which is the **tfidf_score**. `reverse=True` determines that the order should be descending.\n",
    "\n",
    "`train_titles, train_contents, train_labels, _ = zip(*train_data_with_scores)` now the data is sorted using the tfidf scores, so they can be ignored and the data can now be used for curriculum learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wtert\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Fold 1 =====\n",
      "Training label distribution: Counter({2: 9156, 1: 8670, 0: 7210})\n",
      "Validation label distribution: Counter({2: 4578, 1: 4335, 0: 3605})\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15645' max='15645' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15645/15645 1:34:26, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.553218</td>\n",
       "      <td>0.778719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.537667</td>\n",
       "      <td>0.834319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.565283</td>\n",
       "      <td>0.854689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.629572</td>\n",
       "      <td>0.871146</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Accuracy: 0.8711\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.85      0.85      3605\n",
      "           1       0.87      0.87      0.87      4335\n",
      "           2       0.89      0.89      0.89      4578\n",
      "\n",
      "    accuracy                           0.87     12518\n",
      "   macro avg       0.87      0.87      0.87     12518\n",
      "weighted avg       0.87      0.87      0.87     12518\n",
      "\n",
      "\n",
      "===== Fold 2 =====\n",
      "Training label distribution: Counter({2: 9156, 1: 8670, 0: 7210})\n",
      "Validation label distribution: Counter({2: 4578, 1: 4335, 0: 3605})\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15645' max='15645' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15645/15645 1:30:46, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.286108</td>\n",
       "      <td>0.902540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.304475</td>\n",
       "      <td>0.906535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.335786</td>\n",
       "      <td>0.921473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.345463</td>\n",
       "      <td>0.934414</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Accuracy: 0.9344\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.92      0.92      3605\n",
      "           1       0.93      0.94      0.94      4335\n",
      "           2       0.95      0.94      0.95      4578\n",
      "\n",
      "    accuracy                           0.93     12518\n",
      "   macro avg       0.93      0.93      0.93     12518\n",
      "weighted avg       0.93      0.93      0.93     12518\n",
      "\n",
      "\n",
      "===== Fold 3 =====\n",
      "Training label distribution: Counter({2: 9156, 1: 8670, 0: 7210})\n",
      "Validation label distribution: Counter({2: 4578, 1: 4335, 0: 3605})\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15645' max='15645' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15645/15645 1:30:46, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.276568</td>\n",
       "      <td>0.910369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.200979</td>\n",
       "      <td>0.947276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.164488</td>\n",
       "      <td>0.961975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.133224</td>\n",
       "      <td>0.974996</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Accuracy: 0.9764\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97      3605\n",
      "           1       0.97      0.98      0.97      4335\n",
      "           2       0.98      0.98      0.98      4578\n",
      "\n",
      "    accuracy                           0.98     12518\n",
      "   macro avg       0.98      0.98      0.98     12518\n",
      "weighted avg       0.98      0.98      0.98     12518\n",
      "\n",
      "\n",
      "===== Cross-Validation Complete =====\n",
      "Mean Accuracy: 0.9273\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from transformers import AutoTokenizer, DebertaV2ForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import StratifiedKFold as skf\n",
    "from collections import Counter\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\", num_labels=3)\n",
    "\n",
    "class NewsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "class CurriculumTrainer(Trainer):\n",
    "    def get_train_dataloader(self):\n",
    "        # Disable shuffling for curriculum learning\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.args.train_batch_size,\n",
    "            shuffle=False,\n",
    "            collate_fn=self.data_collator,\n",
    "            drop_last=False,\n",
    "        )\n",
    "\n",
    "titles = df[\"cleaned_title\"].tolist()\n",
    "labels = df[\"label_id\"].tolist()\n",
    "accuracies = []  # Initialize accuracies list for storing fold results\n",
    "\n",
    "stratified_kfold = skf(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "model = DebertaV2ForSequenceClassification.from_pretrained(\"microsoft/deberta-v3-base\", num_labels=3)\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(stratified_kfold.split(titles, labels)):\n",
    "    print(f\"\\n===== Fold {fold + 1} =====\")\n",
    "\n",
    "    # STEP 1: Grab training data using the indices from StratifiedKFold\n",
    "    train_data = [\n",
    "        (titles[i], df[\"cleaned_content\"].iloc[i], labels[i]) for i in train_index\n",
    "    ]\n",
    "\n",
    "    # STEP 2: Unpack (unzip) train_data into individual lists\n",
    "    train_titles, train_contents, train_labels = zip(*train_data)\n",
    "\n",
    "    # STEP 3: Combine title and content for each sample\n",
    "    train_texts_full = [f\"{title} {content}\" for title, content in zip(train_titles, train_contents)]\n",
    "\n",
    "    # STEP 4: Compute TF-IDF scores for all the combined texts\n",
    "    tfidf_scores = compute_tfidf_scores(train_texts_full)\n",
    "\n",
    "    # STEP 5: Attach the scores to the data for sorting\n",
    "    train_data_with_scores = list(zip(train_titles, train_contents, train_labels, tfidf_scores))\n",
    "\n",
    "    # STEP 6: Sort the data — lowest TF-IDF (most ambiguous) comes last in curriculum\n",
    "    train_data_with_scores.sort(key=lambda x: x[3], reverse=True)\n",
    "\n",
    "    # STEP 7: Unpack the sorted data (ignore the scores now)\n",
    "    train_titles, train_contents, train_labels, _ = zip(*train_data_with_scores)\n",
    "\n",
    "    val_titles = [titles[i] for i in val_index]\n",
    "    val_contents = [df[\"cleaned_content\"].iloc[i] for i in val_index]\n",
    "    val_labels = [labels[i] for i in val_index]\n",
    "\n",
    "    print(f\"Training label distribution: {Counter(train_labels)}\")\n",
    "    print(f\"Validation label distribution: {Counter(val_labels)}\")\n",
    "\n",
    "    train_texts = [truncate_for_model(t, c) for t, c in zip(train_titles, train_contents)]\n",
    "    val_texts = [truncate_for_model(t, c) for t, c in zip(val_titles, val_contents)]\n",
    "\n",
    "    train_encodings = tokenizer(train_texts, max_length=384, truncation=True, padding=True, return_tensors='pt')\n",
    "    val_encodings = tokenizer(val_texts, max_length=384, truncation=True, padding=True, return_tensors='pt')\n",
    "\n",
    "    train_dataset = NewsDataset(train_encodings, train_labels)\n",
    "    val_dataset = NewsDataset(val_encodings, val_labels)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./results_fold_{fold}\",\n",
    "        num_train_epochs=5,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        warmup_steps=100,\n",
    "        weight_decay=0.1,\n",
    "        logging_dir=None,\n",
    "        logging_steps=999999,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_accuracy\",\n",
    "        save_total_limit=1,\n",
    "        report_to=\"none\",\n",
    "        learning_rate=3e-5,\n",
    "        fp16=True,\n",
    "        lr_scheduler_type='cosine',\n",
    "        gradient_accumulation_steps=2\n",
    "    )\n",
    "\n",
    "    def compute_accuracy(pred):\n",
    "        labels = pred.label_ids\n",
    "        preds = np.argmax(pred.predictions, axis=1)\n",
    "        accuracy = (preds == labels).mean()\n",
    "        return {\"accuracy\": accuracy}\n",
    "\n",
    "    trainer = CurriculumTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_accuracy\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    eval_result = trainer.evaluate()\n",
    "    trainer.save_model(f\"saved_models/fold_{fold}\")\n",
    "    tokenizer.save_pretrained(f\"saved_models/fold_{fold}\")\n",
    "    print(f\"Fold {fold + 1} Accuracy: {eval_result['eval_accuracy']:.4f}\")\n",
    "    accuracies.append(eval_result['eval_accuracy'])\n",
    "\n",
    "    # Generate predictions and classification report\n",
    "    predictions = trainer.predict(val_dataset)\n",
    "    pred_labels = np.argmax(predictions.predictions, axis=1)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(val_labels, pred_labels))\n",
    "\n",
    "print(\"\\n===== Cross-Validation Complete =====\")\n",
    "print(f\"Mean Accuracy: {np.mean(accuracies):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔎 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(val_labels, pred_labels)\n",
    "\n",
    "# Normalize the confusion matrix to show percentages\n",
    "cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "\n",
    "# Display the confusion matrix with percentages\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_percentage, display_labels=label_encoder.classes_)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix (Percentages)\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report (Content Only):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.89      0.87      3605\n",
      "           1       0.91      0.81      0.86      4335\n",
      "           2       0.84      0.90      0.87      4578\n",
      "\n",
      "    accuracy                           0.87     12518\n",
      "   macro avg       0.87      0.87      0.87     12518\n",
      "weighted avg       0.87      0.87      0.87     12518\n",
      "\n",
      "Accuracy (Content Only): 0.8660329126058476\n"
     ]
    }
   ],
   "source": [
    "# Load the best model\n",
    "model = DebertaV2ForSequenceClassification.from_pretrained(\"../models/DeBERTa_97\")\n",
    "\n",
    "# Just use content for validation now\n",
    "content_only_val_texts = val_contents  # Don't prepend title\n",
    "\n",
    "# Tokenize only content\n",
    "content_only_val_encodings = tokenizer(content_only_val_texts, max_length=384, truncation=True, padding=True, return_tensors='pt')\n",
    "\n",
    "# Wrap in Dataset\n",
    "content_only_val_dataset = NewsDataset(content_only_val_encodings, val_labels)\n",
    "\n",
    "# Put model in eval mode\n",
    "model.eval()\n",
    "\n",
    "# Move to the same device as used in training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    all_logits = []\n",
    "    for i in range(0, len(content_only_val_dataset), 16):  # Use reasonable batch size\n",
    "        batch = content_only_val_dataset[i:i+16]\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        all_logits.append(logits.cpu())\n",
    "\n",
    "    all_logits = torch.cat(all_logits)\n",
    "    preds = torch.argmax(all_logits, dim=1).numpy()\n",
    "\n",
    "# Evaluate\n",
    "print(\"Classification Report (Content Only):\")\n",
    "print(classification_report(val_labels, preds))\n",
    "print(\"Accuracy (Content Only):\", accuracy_score(val_labels, preds))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
